{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport cv2\nfrom keras.layers import LeakyReLU\nfrom keras import regularizers\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, AveragePooling2D, Activation, Reshape, Concatenate\nfrom keras.layers.experimental.preprocessing import RandomContrast # random contrast change\n\nfrom keras.utils.np_utils import to_categorical\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils import plot_model\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint, CSVLogger, TensorBoard, EarlyStopping, ReduceLROnPlateau\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport dlib\nimport cv2\nfrom PIL import Image\nfrom keras.preprocessing import image\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import MiniBatchKMeans\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Face Detection:","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def detect_face(image):\n    face_detector = dlib.get_frontal_face_detector()\n    dets = face_detector(image, 1)\n\n    result = []\n    for d in dets:\n        xmin, ymin, xmax, ymax = d.left(), d.top(), d.right(), d.bottom()\n        xmin = max(xmin, 0)\n        xmax = max(xmax, 0)\n        ymin = max(ymin, 0)\n        ymax = max(ymax, 0)\n        result.append([xmin, xmax, ymin, ymax])\n    return result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Filter data set use face detection:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def filter_data_set(filePath, save_path):\n    file_lst = os.listdir(filePath)\n    print(file_lst)\n    for file in file_lst:\n        image_name = file[:-4]\n        test_image = cv2.imread(filePath + file)\n        result = detect_face(test_image)\n        num_face = len(result)\n        print(\"Number of face: %d\" % (num_face))\n        if num_face == 1:\n            xmin, xmax, ymin, ymax = result[0]\n            # test_image = cv2.rectangle(test_image, (xmin, ymin), (xmax, ymax), (0, 0, 255), 2)\n            cropped = test_image[ymin:ymax, xmin:xmax]\n            img = Image.fromarray(cropped)\n            img = np.array(img.resize(size=(48, 48)))\n            cv2.imwrite(save_path + image_name + \".jpg\", img)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This will generate the new data set, do not run this, it will take long, you can use \"../input/new-train-data/new_train/\" and \"../input/new-test-data/new_test/\" directly."},{"metadata":{"trusted":true},"cell_type":"code","source":"expression_lst = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\nsave_path = 'new_train/surprise/'\nfor expression in expression_lst:\n    filePath = '../input/fer2013/train/' + expression + '/'\n    save_path = '../input/new-train-data/new_train/' + expression + '/'\n    filter_data_set(filePath, save_path)\n    filePath = '../input/fer2013/test/' + expression + '/'\n    save_path = '../input/new-test-data/new_test/' + expression + '/'\n    filter_data_set(filePath, save_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**This is for new data set filtered with face detection.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_generator = ImageDataGenerator(rescale=1.0/255.0, zoom_range=0.3, horizontal_flip=True)\n\ntrain_data = train_data_generator.flow_from_directory(\"../input/new-train-data/new_train\", \n                                                      target_size = (48, 48),\n                                                      color_mode = 'grayscale',\n                                                      batch_size = 64,\n                                                      class_mode = 'categorical',\n                                                      shuffle = True,\n                                                      seed = 0)\n\ntest_data_generator = ImageDataGenerator(rescale=1.0/255.0)\ntest_data = test_data_generator.flow_from_directory(\"../input/new-test-data/new_test\", \n                                                    target_size = (48, 48),\n                                                    color_mode = 'grayscale',\n                                                    batch_size = 64,\n                                                    class_mode = 'categorical',\n                                                    shuffle = True, \n                                                    seed = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SIFT/ORB feature extraction for Hybrid CNN (Nizar)"},{"metadata":{"trusted":true},"cell_type":"code","source":"os.mkdir('/kaggle/working/orb_train/')\nos.mkdir('/kaggle/working/orb_test/')\nexpressions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\nfor expression in expressions:\n    save_path = '/kaggle/working/orb_train/' + expression\n    os.mkdir(save_path)\n    save_path = '/kaggle/working/orb_test/' + expression\n    os.mkdir(save_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"orb=cv2.ORB_create()\norb.setEdgeThreshold(5)\ndef compute_ORB(img):\n    kp = orb.detect(img, None)\n    kp, des = orb.compute(img, kp)\n    return des\n\ndef ORB_BoF(k):\n    \"\"\" \n    k represents number of clusters for bag of features. \n    \"\"\"\n    expressions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n    visual_dict = []\n    none_count = 0\n    for expression in expressions:\n        print(expression)\n        dir_name = '../input/new-train-data/new_train/' + expression + '/'\n        for fname in os.listdir(dir_name):\n            fpath = os.path.join(dir_name, fname)\n            img = cv2.imread(fpath, 0)\n            des = compute_ORB(img)\n            if des is not None:\n                visual_dict.append(des)\n            else:\n                print(\"Found none.\")\n                none_count += 1\n    print(f\"Nones: {none_count}\")\n    visual_dict_vector = np.vstack(visual_dict)\n    print(visual_dict_vector.shape)\n    kmeans = MiniBatchKMeans(n_clusters=k, batch_size=128).fit(visual_dict_vector)\n    train_BoF = np.zeros((len(visual_dict), k))\n    n = 0\n    for expression in expressions:\n        print(expression)\n        dir_name = '../input/new-train-data/new_train/' + expression + '/'\n        save_path = '/kaggle/working/orb_train/' + expression +'/'\n        for fname in os.listdir(dir_name):\n            fpath = os.path.join(save_path, fname[:-4]+'.npy')\n            cluster_label = kmeans.predict(visual_dict[n])\n            feature_histogram = np.array([(cluster_label==i).mean() for i in range(k)])\n            np.save(fpath, feature_histogram) # .. save feature_histogram to file\n            train_BoF[n] = feature_histogram\n            n += 1\n    # do same for test set\n    test_BoF = []\n    for expression in expressions:\n        print(expression)\n        dir_name = '../input/new-test-data/new_test/' + expression + '/'\n        save_path = '/kaggle/working/orb_test/' + expression +'/'\n        for fname in os.listdir(dir_name):\n            fpath = os.path.join(dir_name, fname)\n            img = cv2.imread(fpath)\n            des = compute_ORB(img)\n            cluster_label = kmeans.predict(des)\n            feature_histogram = np.array([(cluster_label==i).mean() for i in range(k)])\n            np.save(save_path+fname[:-4]+'.npy', feature_histogram) # .. save feature_histogram to file\n            test_BoF.append(feature_histogram)\n    test_BoF = np.vstack(test_BoF); print(test_BoF.shape)\n    return train_BoF, test_BoF\ntrain_BoF, test_BoF = ORB_BoF(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"expressions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\ndef get_labels():\n    train_labels = []\n    for i, expression in enumerate(expressions):\n        dir_name = '../input/new-train-data/new_train/' + expression + '/'\n        label = np.zeros((len(os.listdir(dir_name))))+i\n        train_labels.append(label)\n    train_labels = np.concatenate(train_labels)\n    test_labels = []\n    for i, expression in enumerate(expressions):\n        dir_name = '../input/new-test-data/new_test/' + expression + '/'\n        label = np.zeros((len(os.listdir(dir_name))))+i\n        test_labels.append(label)\n    test_labels = np.concatenate(test_labels)\n    return train_labels, test_labels\n\ndef one_hot(labels):\n    one_hot_vector = np.zeros((labels.shape[0], 7))\n    for i in range(labels.shape[0]):\n        one_hot_vector[i, int(labels[i])] = 1.\n    return one_hot_vector\n\ny_train, y_test = get_labels()\nY_train, Y_test = one_hot(y_train), one_hot(y_test)\nY_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CNN-ORB Hybrid"},{"metadata":{},"cell_type":"markdown","source":"Here we build the generator for the hybrid model, since it requires two inputs we need to make a custom data generator that takes both inputs."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_orb_generator = ImageDataGenerator()\ntrain_BoF = train_BoF.reshape(train_BoF.shape[0], train_BoF.shape[1],1,1)\ntrain_orb = train_orb_generator.flow(train_BoF, \n                                     Y_train,\n                                      batch_size = 64,\n                                      shuffle = True, \n                                      seed=0)\n\ntest_orb_generator = ImageDataGenerator()\ntest_BoF = test_BoF.reshape(test_BoF.shape[0], test_BoF.shape[1],1,1)\ntest_orb = test_orb_generator.flow(test_BoF, \n                                   Y_test,\n                                    batch_size = 64,\n                                    shuffle = True, \n                                    seed=0)\n\n# adapted from: https://stackoverflow.com/questions/49404993/keras-how-to-use-fit-generator-with-multiple-inputs \nclass JoinedGen(tf.keras.utils.Sequence):\n    def __init__(self, input_gen1, input_gen2):\n        self.gen1 = input_gen1\n        self.gen2 = input_gen2\n        print(len(input_gen1),len(input_gen2))\n        assert len(input_gen1) == len(input_gen2)\n\n    def __len__(self):\n        return self.gen1.__len__()\n\n    def __getitem__(self, index):\n        X1_batch, Y_batch = self.gen1.__getitem__(index)\n        X2_batch, Y_batch = self.gen2.__getitem__(index)\n        X_batch = [X1_batch, X2_batch]\n        return X_batch, Y_batch\n\n    def on_epoch_end(self):\n        self.gen1.on_epoch_end()\n        self.gen2.on_epoch_end()\n        self.gen2.index_array = self.gen1.index_array\ntrain_gen = JoinedGen(train_data, train_orb)\ntest_gen = JoinedGen(test_data, test_orb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model A: CNN with raw image input\nin1 = keras.Input(shape=(48, 48, 1), name=\"img\")\nx = Conv2D(128, kernel_size=(3, 3), padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.1))(in1)\nx = Conv2D(128, kernel_size=(3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding='same')(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D(2, 2)(x)\nx = Dropout(0.1)(x)\nx = Conv2D(256, kernel_size=(3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding='same')(x)\nx = Conv2D(256, kernel_size=(3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1))(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\nx = Dropout(0.1)(x)\nx = Conv2D(512, kernel_size=(3, 3), padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.1))(x)\nx = Conv2D(512, kernel_size=(3, 3), activation= tf.keras.layers.LeakyReLU(alpha=0.1), padding='same')(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D(2, 2)(x)\nx = Dropout(0.1)(x)\nout1 = Flatten()(x)\n\n### Model B: ORB input\nin2 = keras.Input(shape=(100, 1, 1), name=\"orb\")\nintermediate = Reshape((100,))(in2)\nout2 = Dense(4096, activation=tf.keras.layers.LeakyReLU(alpha=0.1))(intermediate)\nout2 = Dropout(0.5)(out2)\n\n# Merged model\nx = Concatenate()([out1, out2])\nout3 = Dense(2048, activation=tf.keras.layers.LeakyReLU(alpha=0.1))(x)\nout3 = Dropout(0.5)(out3)\nout3 = Dense(7, activation='softmax')(out3)\n\n# final model: ORB-CNN hybrid\nmodel = keras.Model([in1, in2], out3, name='ORB-CNN')\nmodel.compile(optimizer=Adam(lr=0.0001, decay=1e-6), \n          loss=\"categorical_crossentropy\", metrics=['accuracy'])\n\nmodel.summary()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(model, to_file='model_visualize.png', show_shapes=True, show_layer_names=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check_point_loss = tf.keras.callbacks.ModelCheckpoint(filepath='model_loss.h5',\n                                                      save_best_only=True,\n                                                      verbose=1,\n                                                      mode='min',\n                                                      moniter='val_loss')\n\ncheck_point_acc = tf.keras.callbacks.ModelCheckpoint(filepath='model_acc.h5',\n                                                      save_weights_only=True,\n                                                      verbose=1,\n                                                      mode='max',\n                                                      moniter='val_accuracy')\n\n# Reduce learning rate when a metric has stopped improving.\nreduce_learning_rate = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n                                                            factor=0.2, \n                                                            patience=5, \n                                                            verbose=1, \n                                                            min_delta=0.0001)\n\n\nlogs = tf.keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=1),\ncsv_logger = tf.keras.callbacks.CSVLogger('training.log')\n\ncallbacks = [check_point_loss,check_point_acc, reduce_learning_rate, csv_logger]\n\nhistory = model.fit(train_gen,\n                    steps_per_epoch=train_data.n // 64,\n                    epochs=20,\n                    validation_data=test_gen,\n                    validation_steps=test_data.n // 64,\n                    callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.title(\"ORB-CNN Hybrid Model Accuracy\")\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.legend(['train', 'test'])\nfig=plt.figure()\nplt.title(\"ORB-CNN Hybrid Model Loss\")\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.legend(['train', 'test'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"End of CNN-ORB Hybrid"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_generator = ImageDataGenerator(rescale=1.0/255.0, zoom_range=0.3, horizontal_flip=True)\n\ntrain_data = train_data_generator.flow_from_directory(\"../input/fer2013/train/\", \n                                                      target_size = (48, 48),\n                                                      color_mode = 'grayscale',\n                                                      batch_size = 64,\n                                                      class_mode = 'categorical',\n                                                      shuffle = True)\n\ntest_data_generator = ImageDataGenerator(rescale=1.0/255.0)\ntest_data = test_data_generator.flow_from_directory(\"../input/fer2013/test/\", \n                                                    target_size = (48, 48),\n                                                    color_mode = 'grayscale',\n                                                    batch_size = 64,\n                                                    class_mode = 'categorical',\n                                                    shuffle = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CNN model:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.models.Sequential()   \n\nmodel.add(Conv2D(128, kernel_size=(3, 3), padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.1), input_shape=(48,48,1)))\nmodel.add(Conv2D(128, kernel_size=(3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(2, 2))\nmodel.add(Dropout(0.1)) #try 0.15\n\nmodel.add(Conv2D(256, kernel_size=(3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding='same'))\nmodel.add(Conv2D(256, kernel_size=(3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.1))\n\nmodel.add(Conv2D(512, kernel_size=(3, 3), padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.1), input_shape=(48,48,1)))\nmodel.add(Conv2D(512, kernel_size=(3, 3), activation= tf.keras.layers.LeakyReLU(alpha=0.1), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(2, 2))\nmodel.add(Dropout(0.1))\n\n\nmodel.add(Flatten())\n\nmodel.add(Dense(512,kernel_regularizer=regularizers.l2(0.01),activation=tf.keras.layers.LeakyReLU(alpha=0.1)))\n#above we get out \nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(7, activation='softmax'))\n\nmodel.compile(optimizer=Adam(lr=0.0001, decay=1e-6), \n          loss=\"categorical_crossentropy\", metrics=['accuracy'])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot CNN model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(model, to_file='model_visualize.png', show_shapes=True, show_layer_names=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check_point_loss = tf.keras.callbacks.ModelCheckpoint(filepath='model_loss.h5',\n                                                      save_best_only=True,\n                                                      verbose=1,\n                                                      mode='min',\n                                                      moniter='val_loss')\n\ncheck_point_acc = tf.keras.callbacks.ModelCheckpoint(filepath='model_acc.h5',\n                                                      save_weights_only=True,\n                                                      verbose=1,\n                                                      mode='max',\n                                                      moniter='val_accuracy')\n\n# Reduce learning rate when a metric has stopped improving.\nreduce_learning_rate = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n                                                            factor=0.2, \n                                                            patience=5, \n                                                            verbose=1, \n                                                            min_delta=0.0001)\n\n\nlogs = tf.keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=1),\ncsv_logger = tf.keras.callbacks.CSVLogger('training.log')\n\ncallbacks = [check_point_loss,check_point_acc, reduce_learning_rate, csv_logger]\n\nhistory = model.fit(train_data,\n                    steps_per_epoch=train_data.n // 64,\n                    epochs=40,\n                    validation_data=test_data,\n                    validation_steps=test_data.n // 64,\n                    callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot accuracy and loss for CNN model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title(\"CNN Model Accuracy\")\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.legend(['train', 'test'])\nfig=plt.figure()\nplt.title(\"CNN Model Loss\")\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.legend(['train', 'test'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Face detection"},{"metadata":{},"cell_type":"markdown","source":"You need to change the image_name and the image path for running different images."},{"metadata":{"trusted":true},"cell_type":"code","source":"image_name = \"test3\"\ntest_image = cv2.imread('../input/face-detection/test3.jpeg', 0)\nh, w = test_image.shape[0], test_image.shape[1]\nprint(test_image.shape)\nresult = detect_face(test_image)\nnum_face = len(result)\nprint(\"Number of face: %d\" % (num_face))\n# num_face\nfor i in range(num_face):\n    xmin, xmax, ymin, ymax = result[i]\n    xmin = min(xmin, w)\n    xmax = min(xmax, w)\n    ymin = min(ymin, h)\n    ymax = min(ymax, h)\n    test_image = cv2.rectangle(test_image, (xmin, ymin), (xmax, ymax), (0, 0, 255), 2)\n    cropped = test_image[ymin:ymax, xmin:xmax]\n    img = Image.fromarray(cropped)\n    img = img.resize(size=(48, 48))\n    plt.imshow(img, cmap='gray')\n    img_input = image.img_to_array(img)\n    img_input = np.expand_dims(img_input, axis=0)\n    img_input /= 255\n    emotion_pribability = model.predict(img_input)\n    print(emotion_pribability)\n    emotion_pred = np.argmax(emotion_pribability, axis=1)\n    print(emotion_pred)\n    class_labels = test_data.class_indices\n    print(class_labels)\n    emotion = [k for k, v in class_labels.items() if v == emotion_pred]\n    print(emotion[0])\n    text_x = xmin\n    text_y = int(ymin + (ymax - ymin) / 2)\n    cv2.putText(test_image, emotion[0], (text_x, text_y), 1, 1, (0, 255, 0), 1)\nplt.imshow(test_image, cmap='gray')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}
