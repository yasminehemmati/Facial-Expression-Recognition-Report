{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom keras.utils import np_utils\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport cv2\nfrom keras.layers import LeakyReLU\nfrom keras import regularizers\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, AveragePooling2D, Activation\nfrom keras.utils.np_utils import to_categorical\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils import plot_model\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint, CSVLogger, TensorBoard, EarlyStopping, ReduceLROnPlateau\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/fer2013/fer2013.csv\")","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Usage'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = df[df['Usage']=='Training']\nPublicTest = df[df['Usage']=='PublicTest']\nPrivateTest = df[df['Usage']== 'PrivateTest']\ntest = pd.concat([PublicTest,PrivateTest])\nprint(len(train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"number of train samples and test samples same as data used in other notebook ","metadata":{}},{"cell_type":"code","source":"train.emotion.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.emotion.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\npixels = df['pixels']\n#print(pixels)\nX = np.zeros((len(pixels), 48*48)) #images 48X48\nfor idx_x in range(X.shape[0]):\n\n    vals = pixels[idx_x].split(' ')\n    #print(len(vals)/48)\n \n    for idx_y in range(X.shape[1]):\n        X[idx_x,idx_y] = int(vals[idx_y])\n    \n        \nX = X/255 #normalizing images","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"im = X[0].copy()\nim = im.reshape((48,48))\nplt.imshow(m)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y = df['emotion']\nY = np_utils.to_categorical(Y, 7) \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index = len(train)\nX_train = X[0:index, :]\nY_train = Y[0:index]\nX_crossval = X[index:, :]\nY_crossval = Y[index:]\n\nprint (X_train.shape, Y_train.shape)\nprint (X_crossval.shape, Y_crossval.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_train.reshape((X_train.shape[0], 48, 48 ,1))\nX_crossval = X_crossval.reshape((X_crossval.shape[0], 48, 48,1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data Augmentation Same as other Notebook","metadata":{}},{"cell_type":"code","source":"datagen = ImageDataGenerator( zoom_range=0.3, horizontal_flip=True)\n\ndatagen.fit(X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    model = tf.keras.models.Sequential()   \n\n    model.add(Conv2D(128, kernel_size=(3, 3), padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.1), input_shape=(48,48,1)))\n    model.add(Conv2D(128, kernel_size=(3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding='same'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(2, 2))\n    model.add(Dropout(0.1)) #try 0.15\n\n    model.add(Conv2D(256, kernel_size=(3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding='same'))\n    model.add(Conv2D(256, kernel_size=(3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1)))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.1))\n    \n    model.add(Conv2D(512, kernel_size=(3, 3), padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.1), input_shape=(48,48,1)))\n    model.add(Conv2D(512, kernel_size=(3, 3), activation= tf.keras.layers.LeakyReLU(alpha=0.1), padding='same'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(2, 2))\n    model.add(Dropout(0.1))\n    \n\n    model.add(Flatten())\n\n    model.add(Dense(512,kernel_regularizer=regularizers.l2(0.01),activation=tf.keras.layers.LeakyReLU(alpha=0.1)))\n    #above we get out \n    model.add(Dropout(0.2))\n\n    model.add(Dense(7, activation='softmax'))\n\n    model.compile(optimizer=Adam(lr=0.0001, decay=1e-6), \n              loss=\"categorical_crossentropy\", metrics=['accuracy'])\n\n    model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 64\nepochs = 50\nsteps_per_epoch = len(X_train) // batch_size\nvalidation_steps = len(X_crossval) // batch_size","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"check_point_loss = tf.keras.callbacks.ModelCheckpoint(filepath='model_loss.h5',\n                                                      save_best_only=True,\n                                                      verbose=1,\n                                                      mode='min',\n                                                      moniter='val_loss')\n\ncheck_point_acc = tf.keras.callbacks.ModelCheckpoint(filepath='model_acc.h5',\n                                                      save_weights_only=True,\n                                                      verbose=1,\n                                                      mode='max',\n                                                      moniter='val_accuracy')\n\n# Reduce learning rate when a metric has stopped improving.\nreduce_learning_rate = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n                                                            factor=0.2, \n                                                            patience=5, \n                                                            verbose=1, \n                                                            min_delta=0.0001)\n\n\nlogs = tf.keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=1),\ncsv_logger = tf.keras.callbacks.CSVLogger('training.log')\n\ncallbacks = [check_point_loss,check_point_acc, reduce_learning_rate, csv_logger]\n\nhist = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=64),\n                           steps_per_epoch= steps_per_epoch,\n                           epochs=50, \n                           verbose=1, \n                           validation_data=(X_crossval, Y_crossval), \n                           callbacks=callbacks)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nfrom keras.models import Model\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier  \nfrom sklearn import metrics\nfrom sklearn import svm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict_classes(X_crossval)\ny_pred\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_actual = df['emotion'][index:]\ny_actual = np.asarray(y_actual)\ny_actual ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics.accuracy_score(y_actual  ,y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Following Code for plotting a confusion matrix from:**\n\nhttps://www.kaggle.com/grfiv4/plot-a-confusion-matrix","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n\ndef plot_confusion_matrix(cm,\n                          target_names,\n                          title='Confusion matrix',\n                          cmap=None,\n                          normalize=True):\n    \"\"\"\n    given a sklearn confusion matrix (cm), make a nice plot\n\n    Arguments\n    ---------\n    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n\n    target_names: given classification classes such as [0, 1, 2]\n                  the class names, for example: ['high', 'medium', 'low']\n\n    title:        the text to display at the top of the matrix\n\n    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n                  see http://matplotlib.org/examples/color/colormaps_reference.html\n                  plt.get_cmap('jet') or plt.cm.Blues\n\n    normalize:    If False, plot the raw numbers\n                  If True, plot the proportions\n\n    Usage\n    -----\n    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n                                                              # sklearn.metrics.confusion_matrix\n                          normalize    = True,                # show proportions\n                          target_names = y_labels_vals,       # list of names of the classes\n                          title        = best_estimator_name) # title of graph\n\n    Citiation\n    ---------\n    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import itertools\n\n    accuracy = np.trace(cm) / float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n\n    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm =confusion_matrix(y_actual, y_pred)\ncm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]\nplot_confusion_matrix(cm = cm, normalize= False,target_names = labels,title = \"Confusion Matrix for CNN\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**CNN + Another Classifier**","metadata":{}},{"cell_type":"code","source":"model_feat = Model(model.input,outputs=model.layers[-3].output)\nx_1 = model_feat.predict(X_train)\ny_1 = np.asarray(df['emotion'][0:index])\nx_2 = model_feat.predict(X_crossval)\ny_2 = np.asarray(df['emotion'][index:])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"One Hot Encoding:\nFollowing Code for One Hot Encoding from https://www.analyticsvidhya.com/blog/2020/03/one-hot-encoding-vs-label-encoding-using-scikit-learn/","metadata":{}},{"cell_type":"code","source":"onehotencoder1 = OneHotEncoder()\n#reshape the 1-D country array to 2-D as fit_transform expects 2-D and finally fit the object \nY1 = onehotencoder1.fit_transform(y_1.reshape(-1,1)).toarray()\nonehotencoder2 = OneHotEncoder()\nY2 = onehotencoder1.fit_transform(y_2.reshape(-1,1)).toarray()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"RFC1: One Hot Encoded\n\nAccuracy = 0.5662533091821095\n\nAccuracy very low so did not use one hot encoding for the rest of the RF  models","metadata":{}},{"cell_type":"code","source":"rfc1 = RandomForestClassifier(random_state=42, n_estimators=300)\nrfc1.fit(x_1, Y1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predrfc = rfc1.predict(x_2)\nmetrics.accuracy_score(predrfc ,Y2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**RFC1: Without One Hot Encoding** \n\nAccuracy: 0.6968092517765083","metadata":{}},{"cell_type":"code","source":"rfc1 = RandomForestClassifier(random_state=42, n_estimators=300)\nrfc1.fit(x_1, y_1)\n\npredrfc = rfc1.predict(x_2)\nmetrics.accuracy_score(predrfc ,y_2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Using Default RFC**\n\nAccuracy:0.6961125818587154","metadata":{}},{"cell_type":"code","source":"rfc2 = RandomForestClassifier()\nrfc2.fit(x_1, y_1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predrfc2 = rfc2.predict(x_2)\nmetrics.accuracy_score(predrfc2 ,y_2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"RFC parameters 3\n\nAccuracy:0.6969485857600669","metadata":{}},{"cell_type":"code","source":"rfc3 = RandomForestClassifier(random_state=42, n_estimators=700)\nrfc3.fit(x_1, y_1)\n\npredrfc3 = rfc3.predict(x_2)\nmetrics.accuracy_score(predrfc3 ,y_2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"RFC parameters 4:\n\nAccuracy:0.6937439041382193\n","metadata":{}},{"cell_type":"code","source":"rfc4 = RandomForestClassifier(random_state=42, n_estimators=1200)\nrfc4.fit(x_1, y_1)\n\npredrfc4 = rfc4.predict(x_2)\nmetrics.accuracy_score(predrfc4 ,y_2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"XGB classifier\nAccuracy 0.6910965584506061","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\nxgb1 =  XGBClassifier(n_estimators = 700,learning_rate=0.01)\nxgb1.fit(x_1, y_1)\ny_pred1 = xgb1.predict(x_2)\nmetrics.accuracy_score(y_2, y_pred1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SVM rbf Kernal\n\nAccuracy: 0.6862198690260555 (OVR)\n\nAccuracy : 0.6912358924341647 (OVO)","metadata":{}},{"cell_type":"code","source":"rbf_svc = svm.SVC(kernel='rbf',decision_function_shape='ovo')\nrbf_svc.fit(x_1, y_1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_rbf_svc = rbf_svc.predict(x_2)\nmetrics.accuracy_score(pred_rbf_svc ,y_2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SVM Linear Kernal\n\nAccuracy: 0.6742371464400168","metadata":{}},{"cell_type":"code","source":"linear_svc = svm.SVC(kernel='linear',decision_function_shape='ovo')\nlinear_svc.fit(x_1, y_1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_linear_svc = linear_svc.predict(x_2)\nmetrics.accuracy_score(pred_linear_svc ,y_2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SVM Linear Kernal (ovr):\n\nAccuracy: 0.6637870976731225\n","metadata":{}},{"cell_type":"code","source":"linear_svc1 = svm.SVC(kernel='linear',decision_function_shape='ovr')\nlinear_svc1.fit(x_1, y_1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_linear_svc1 = linear_svc1.predict(x_2)\nmetrics.accuracy_score(pred_linear_svc1 ,y_2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}